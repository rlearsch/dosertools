import numpy as np
import pandas as pd
import typing

from scipy.optimize import curve_fit
from scipy.optimize import fsolve
from scipy import stats

import data_processing.array as dparray
import data_processing.csv as process

def find_EC_slope(run_dataset: pd.DataFrame, start: float, end: float) -> typing.Tuple[float, float, float]:
    """
    Finds the exponential decay of the EC region for a single dataset. Also returns the intercept and r value of the fit

    Parameters
    ----------
    run_dataset: pd.DataFrame
        dataset of a single DOS run containing at least R/R0, time,
    start: float
        value of R/R0 to start fitting the EC region from
    end: float
        value of R/R0 to end fitting the EC region with

    Returns
    -------
    slope, intercept, r_value: floats
        slope is the slope of the semilog-transformed version of the dataset. It corresponds to the decaying exponential in the linear-linear version of the data
    """
    start_index = dparray.closest_index_for_value(run_dataset, "R/R0", start)
    end_index = dparray.closest_index_for_value(run_dataset, "R/R0", end)
    dataset_EC = run_dataset[start_index:end_index]
    log_radius = np.log(dataset_EC['R/R0'])
    slope, intercept, r_value, p_value, std_err = stats.linregress(dataset_EC['time (s)'],log_radius)
    return slope, intercept, r_value

def annotate_summary_df(fitting_results_list: list) -> pd.DataFrame:
    """
    Do we want to bring other columns with us like ion, polymer identity, etc? How to code that?

    Parameters
    ----------
    fitting_results_list: list
        generated by find_EC_slope
    original_df : pd.DataFrame
        Contains R/R0, time, t - tc, strain rate, R(tc)/R0, etc for multiple runs and samples

    Returns
    -------
    lambdaE_df : pd.DataFrame
        dataframe containing lambdaE relaxation time for each run from the input df
    """
    lambdaE_df = pd.DataFrame(fitting_results_list)
    lambdaE_df = lambdaE_df.rename(columns={0:"sample", 1:"-b", 2:"Intercept", 3:"R",4:"run",5:"Rtc/R0"})
    lambdaE_df['Lambda E (s)'] = -1/(3*lambdaE_df['-b'])
    lambdaE_df['Lambda E (ms)'] = lambdaE_df['Lambda E (s)']*1000
    lambdaE_df['R^2'] = (lambdaE_df['R'])**2
    lambdaE_df = lambdaE_df.drop(["-b","Intercept","R","Lambda E (s)", ],axis=1)
    return lambdaE_df

def make_summary_dataframe(df: pd.DataFrame, fitting_bounds: typing.Tuple[float, float] = [0.1, 0.045]) -> pd.DataFrame:
    """
    Condenses a DOS run into an extensional relaxation time by fitting the EC region (t > tc) to a decaying exponential

    Parameters
    ----------
    df : pd.DataFrame
        Contains R/R0, time, t - tc, strain rate, R(tc)/R0, etc for multiple runs and samples
    fitting_bounds: list, optional
        [start, end]
        These are the R/R0 values we look for to set the bounds for the EC region fitting

    Returns
    -------
    lambdaE_df : pd.DataFrame
        dataframe containing lambdaE relaxation time for each run from the input df
    """
    start = fitting_bounds[0]
    end = fitting_bounds[1]
    fitting_results_list = []
    samples = df["sample"].unique()
    for sample in samples:
        sample_dataset = df[(df["sample"] == sample)]
        run_values = sample_dataset['run'].unique()
        for run in run_values:
            run_dataset = sample_dataset[(sample_dataset['run'] == run)]
            run_dataset = run_dataset.reset_index(drop=True)
            R_tc_R0 = run_dataset.loc[0, "Rtc/R0"]
            fitting_results_temp =  [sample, *find_EC_slope(run_dataset, start, end),run, R_tc_R0]
            fitting_results_list.append(fitting_results_temp)
    #### Clean up the dataframe column names ###
    summary_df = annotate_summary_df(fitting_results_list)
                                     ### Save the df as a csv later in integration ? ###
    return summary_df


    # Reads in parameters from file name and add to dataframe.
    fname = Path(csv).name
    params = fh.folder.parse_filename(fname,fname_format,sampleinfo_format,fname_split,sample_split)
    for key, value in params.items():
        dataset[key] = value